# -*- coding: utf-8 -*-
"""
Created on Sun Mar 15 17:27:08 2020

@author: eva
"""


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib as mpl


X = np.loadtxt('linear-regression.txt',delimiter=',')
y = X[:,2]

class LinearRegression:
    
    def __init__(self,X,y):
        """
        X: shape(N,d)
        y: shape(N,)
        """
        self.X = X
        self.N = self.X.shape[0]
        self.X = np.hstack((np.ones((self.N,1)),self.X))
        self.y = y
        

    def Ein(self,w):
        return (self.X@w-self.y).T@(self.X@w-self.y)/self.N


    def fit(self,eta = 0.1,tol = 0.000001,max_iter=7000):
        self.iter = 0
        self.init_w = np.random.randint(-3,3,size = (self.X.shape[1]))
        self.loss = []
        prev_loss = float('inf')
        self.min_loss = float('inf')
        w = self.init_w
        diff = -1
        ada = np.zeros_like(self.init_w)
        while self.iter<max_iter and abs(diff)>tol:
            
            l = self.Ein(w)
            self.loss.append(l)
            diff,prev_loss = l - prev_loss,l
            if l < self.min_loss:
                self.min_loss,self.gradient_estimate = l,w
            gradient_Ein = 2*self.X.T@(self.X@w-self.y)/self.N
            ada = (ada **2+gradient_Ein**2)**0.5
            w = w -eta*gradient_Ein
            self.iter+=1

        self.best_estimate = np.linalg.inv(self.X.T@self.X)@self.X.T@self.y
       
    def iteration_result(self):
        fig, ax0 = plt.subplots()
        ax0.set_title('loss(Ein)')
        ax0.plot(range(len(self.loss)),np.array(self.loss),lw = 1)  
    
    
    
    def predict(self,data):
        z = data@self.best_estimate
        return z
    
    def plot_result(self,w = []):
        
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        
        if list(w):
            xx, yy = np.meshgrid(np.arange(0,1.1,0.1), np.arange(0,1.1,0.1))
            z = (w[1]*xx+w[2]*yy+w[0])*1.0
            ax.plot_surface(xx,yy,z,alpha = 0.6,color='tab:green')
    
        ax.scatter(self.X[:,1],self.X[:,2],self.y)
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        plt.show()

model = LinearRegression(X[:,:2],X[:,2])
model.fit(eta = 0.01,tol = 0.00000001,max_iter=10000)

model.iteration_result()


print("# of iterations (using gradient descent):",model.iter)
print("weights generated by gradient descent method",model.gradient_estimate)
print("weights (numerical answer):",model.best_estimate)


model.min_loss
model.plot_result(model.gradient_estimate)

